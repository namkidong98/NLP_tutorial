{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "17xj3TusNvIu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtnr-w8XOVbd",
        "outputId": "fd041794-31a6-47ae-e35a-136d5568dfc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-30 00:48:01--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  17.3MB/s    in 0.4s    \n",
            "\n",
            "2024-08-30 00:48:02 (17.3 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ]
        }
      ],
      "source": [
        "# 기계 번역에 사용할 데이터셋은 왼쪽의 영어 문장과 오른쪽의 프랑스어 문장을 tab으로 구분한 형식(19만개의 병렬 문장 샘플)\n",
        "!wget -c http://www.manythings.org/anki/fra-eng.zip && unzip -o fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uJkqNFlaOX7k"
      },
      "outputs": [],
      "source": [
        "def unicode_to_ascii(s):\n",
        "  # 프랑스어 악센트(accent) 삭제\n",
        "  # 예시 : 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QBNeJmQNOZfY"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sent):\n",
        "  # 악센트 삭제 함수 호출\n",
        "  sent = unicode_to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백을 만듭니다.\n",
        "  # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XlGih0rjObEi"
      },
      "outputs": [],
      "source": [
        "num_samples = 33000\n",
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t') # tab을 기준으로 source(영어)와 target(프랑스어)로 구분\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()] # Decoder의 입력(프랑스어 입력)으로는 <sos>가 맨 앞에 추가된다\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()] # Decoder의 출력(프랑스어 출력)으로는 <eos>가 마지막에 추가된다\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRkhuQmZOc4J",
        "outputId": "121bcc8a-0bc9-477c-b1b5-43d929a4840a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
          ]
        }
      ],
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D0HMayt2O33k"
      },
      "outputs": [],
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO83jWzSO88Q",
        "outputId": "44965434-e6a4-4b11-f580-63d4bbbadc23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
            "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
            "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
          ]
        }
      ],
      "source": [
        "print('인코더의 입력 :',sents_en_in[:5])\n",
        "print('디코더의 입력 :',sents_fra_in[:5])\n",
        "print('디코더의 레이블 :',sents_fra_out[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6B26J9n3PGKm"
      },
      "outputs": [],
      "source": [
        "def build_vocab(sents):\n",
        "  # 사전 만들기\n",
        "  word_list = []\n",
        "  for sent in sents:\n",
        "      for word in sent:\n",
        "        word_list.append(word)\n",
        "\n",
        "  # 각 단어별 등장 빈도를 계산하여 등장 빈도가 높은 순서로 정렬\n",
        "  word_counts = Counter(word_list)\n",
        "  vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "  # 사전을 바탕으로 단어를 정수 인덱스로 인코딩하는 딕셔너리 생성\n",
        "  word_to_index = {}\n",
        "  word_to_index['<PAD>'] = 0\n",
        "  word_to_index['<UNK>'] = 1\n",
        "\n",
        "  # 등장 빈도가 높은 단어일수록 낮은 정수를 부여\n",
        "  for index, word in enumerate(vocab) :\n",
        "    word_to_index[word] = index + 2\n",
        "\n",
        "  return word_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP5XdVblPX6z",
        "outputId": "582bdce5-fa8d-45cd-f819-3033afffaac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어 집합의 크기 : 4486, 프랑스어 단어 집합의 크기 : 7879\n"
          ]
        }
      ],
      "source": [
        "src_vocab = build_vocab(sents_en_in)\n",
        "tar_vocab = build_vocab(sents_fra_in + sents_fra_out) # 프랑스어 입력과 프랑스어 출력 문장들로 vocab을 만들고 정수 인덱스 인코딩을 만든다\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "tar_vocab_size = len(tar_vocab)\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O2Fl176UPZLw"
      },
      "outputs": [],
      "source": [
        "# 영어와 프랑스어 모두 word_to_index의 반대인 정수 인덱스로부터 원래의 문장을 복원(Decoding)하는 딕셔너리 생성\n",
        "index_to_src = {v: k for k, v in src_vocab.items()}\n",
        "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
        "\n",
        "def texts_to_sequences(sents, word_to_index):\n",
        "  encoded_X_data = []\n",
        "  for sent in tqdm(sents):\n",
        "    index_sequences = []\n",
        "    for word in sent:\n",
        "      try:\n",
        "          index_sequences.append(word_to_index[word])\n",
        "      except KeyError: # 사전에 없는 단어는 Unk 토큰의 정수 인덱스(1번)으로 변환\n",
        "          index_sequences.append(word_to_index['<UNK>'])\n",
        "    encoded_X_data.append(index_sequences)\n",
        "  return encoded_X_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4I6JyT3Paci",
        "outputId": "8700f6e4-882f-4b3b-d705-0a969bb55150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33000/33000 [00:00<00:00, 523397.83it/s]\n",
            "100%|██████████| 33000/33000 [00:00<00:00, 429498.90it/s]\n",
            "100%|██████████| 33000/33000 [00:00<00:00, 127302.12it/s]\n"
          ]
        }
      ],
      "source": [
        "encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n",
        "decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n",
        "decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oA1o5pdPblr",
        "outputId": "10bea4e3-1a07-4b76-c44a-a24df96b1da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [736, 2]\n"
          ]
        }
      ],
      "source": [
        "# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n",
        "# 인코더 입력이므로 <sos>나 <eos>가 없음\n",
        "for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n",
        "    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fkUn1iENPc3-"
      },
      "outputs": [],
      "source": [
        "def pad_sequences(sentences, max_len=None):\n",
        "    # 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n",
        "    if max_len is None:\n",
        "        max_len = max([len(sentence) for sentence in sentences])\n",
        "\n",
        "    # 최대길이까지 0으로 초기화(0은 PAD 토큰)\n",
        "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
        "    for index, sentence in enumerate(sentences):\n",
        "        if len(sentence) != 0:\n",
        "            features[index, :len(sentence)] = np.array(sentence)[:max_len] # 최대 길이만큼 기존 문장의 인코딩된 정수 인덱스 sequence를 덮씌운다\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-0lkT1_Pfvy",
        "outputId": "e98c3040-54c9-478e-e6ff-26f929a905ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력의 크기(shape) : (33000, 7)\n",
            "디코더의 입력의 크기(shape) : (33000, 16)\n",
            "디코더의 레이블의 크기(shape) : (33000, 16)\n"
          ]
        }
      ],
      "source": [
        "# 문장들을 정수 인덱스들의 시퀀스로 변환해둔 상태\n",
        "# 이번에는 padding을 추가하여 길이를 맞춰준다\n",
        "encoder_input = pad_sequences(encoder_input)\n",
        "decoder_input = pad_sequences(decoder_input)\n",
        "decoder_target = pad_sequences(decoder_target)\n",
        "\n",
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaMUu5kPPgzP",
        "outputId": "b406e1d6-24a3-4bd0-fd96-f540626ec310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시퀀스 : [19443 10350  3505 ... 31503  7147   488]\n"
          ]
        }
      ],
      "source": [
        "indices = np.arange(encoder_input.shape[0]) # 33000을 범위로 하는 index 리스트를 만들고\n",
        "np.random.shuffle(indices)                  # shuffle해서 뒤섞는다\n",
        "print('랜덤 시퀀스 :',indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtqJpgiKPi4f",
        "outputId": "e817d51b-04fc-4a39-bbea-0218377c2564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tom', 'was', 'invited', '.', '<PAD>', '<PAD>', '<PAD>']\n",
            "['<sos>', 'tom', 'a', 'ete', 'invite', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['tom', 'a', 'ete', 'invite', '.', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ],
      "source": [
        "# 뒤섞은 index를 이용해서 encoder_input, decoder_input, decoder_target을 모두 뒤섞어준다\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n",
        "\n",
        "# 랜덤한 인덱스에 대해 값을 찍어본 결과\n",
        "print([index_to_src[word] for word in encoder_input[30997]]) # encoder의 입력(영어 문장)\n",
        "print([index_to_tar[word] for word in decoder_input[30997]]) # decoder의 입력(<sos>로 시작하는 프랑스어 문장)\n",
        "print([index_to_tar[word] for word in decoder_target[30997]]) # decoder의 출력(<eos>로 끝나는 프랑스어 문장)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDwZXlVPlU8",
        "outputId": "9a805985-3397-44a0-dec8-2f8d4d2c6bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 데이터의 개수 : 3300\n"
          ]
        }
      ],
      "source": [
        "n_of_val = int(33000*0.1)\n",
        "print('검증 데이터의 개수 :',n_of_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MbLN9dQ4Pnel"
      },
      "outputs": [],
      "source": [
        "# 뒤에서 3300개 전까지를 train으로 잡고\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "# 뒤의 3300개를 test로 잡는다\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k3JRz3kPn6I",
        "outputId": "bfad1666-971b-4747-8457-1792ec366052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 source 데이터의 크기 : (29700, 7)\n",
            "훈련 target 데이터의 크기 : (29700, 16)\n",
            "훈련 target 레이블의 크기 : (29700, 16)\n",
            "테스트 source 데이터의 크기 : (3300, 7)\n",
            "테스트 target 데이터의 크기 : (3300, 16)\n",
            "테스트 target 레이블의 크기 : (3300, 16)\n"
          ]
        }
      ],
      "source": [
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fNtfGqVPrIw"
      },
      "source": [
        "## Machine Translation (Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ptvan86aPpcC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "embedding_dim = 256 # Seq2Seq를 이루는 LSTM에 입력으로 들어가기 전 단어를 256차원의 임베딩 값으로 변환\n",
        "hidden_units = 256  # LSTM의 각 time step의 hidden state의 dimension(=hidden_size)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0) # 0번 인덱스를 padding으로\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape == (batch_size, seq_len, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        # hidden.shape == (1, batch_size, hidden_units), cell.shape == (1, batch_size, hidden_units)\n",
        "        _, (hidden, cell) = self.lstm(x)\n",
        "        # 인코더의 출력은 hidden state, cell state\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, tar_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
        "\n",
        "        # lstm의 마지막 layer의 출력 dim은 hidden state의 dim(=hidden size)로 이것이 출력층인 Fully Connected의 입력이 되고\n",
        "        # 출력층의 출력은 target인 프랑스어의 어휘 개수가 된다\n",
        "        self.fc = nn.Linear(hidden_units, tar_vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = self.embedding(x) # x.shape == (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # 디코더의 LSTM으로 인코더의 hidden state, cell state를 전달.\n",
        "        # output.shape == (batch_size, seq_len, hidden_units)\n",
        "        # hidden.shape == (1, batch_size, hidden_units)\n",
        "        # cell.shape == (1, batch_size, hidden_units)\n",
        "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
        "\n",
        "        # output.shape: (batch_size, seq_len, tar_vocab_size)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        # 디코더의 출력은 예측값, hidden state, cell state\n",
        "        return output, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        hidden, cell = self.encoder(src)    # Encoder를 통해 hidden state와 cell state를 계산하고\n",
        "                                            # 그 hidden이 Decoder의 첫 입력에 주어진다\n",
        "        output, _, _ = self.decoder(trg, hidden, cell) # 훈련 중에는 디코더의 출력 중 오직 output만 사용(Teacher Forcing으로 다음 입력은 주어진다)\n",
        "        return output\n",
        "\n",
        "encoder = Encoder(src_vocab_size, embedding_dim, hidden_units)\n",
        "decoder = Decoder(tar_vocab_size, embedding_dim, hidden_units)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0) # PAD 토큰의 정수 인덱스인 0번에 대한 손실 계산은 무시\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6RhONZxPtXs",
        "outputId": "addb9996-65a5-40bf-be85-6476c3c7342c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(4486, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(7879, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "    (fc): Linear(in_features=256, out_features=7879, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvgcuYeOwAww"
      },
      "source": [
        "- 영어 문장은 Encoder에 입력되는데 영어 문장들을 토크나이징하여 얻은 영어 사전의 어휘 개수인 4486이 원핫인코딩된 정수 인덱스를 의미하며 이것이 embedding 차원인 256으로 변환된다\n",
        "- Encoder에서 LSTM에 입력으로 임베딩된 256차원의 입력이 들어와서 256차원의 hidden state의 dimension이 반환된다\n",
        "- Decoder에서는 Encoder의 hidden state와 함께 target인 프랑스 문장을 정수 인코딩했다가 256차원의 임베딩된 값을 입력 받는다\n",
        "- Decoder의 LSTM에 256차원의 입력이 들어오고 이를 256차원의 hidden state로 반환한다\n",
        "- Decoder에서 매 time step마다의 hidden state를 Fully Connected Layer에 집어넣어서 target인 프랑스 사전의 어휘 개수인 7879차원에 맞게 반환한다\n",
        "- 각 차원에 있는 로짓값은 해당 time step에서 계산된 hidden state를 바탕으로 프랑스어 사전에서 각 단어에 대한 로짓값이 된다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PHGcTybqPuUN"
      },
      "outputs": [],
      "source": [
        "def evaluation(model, dataloader, loss_function, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    # gradient 계산 및 업데이트 없이\n",
        "    with torch.no_grad():\n",
        "        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n",
        "            # 데이터를 꺼내오고\n",
        "            encoder_inputs = encoder_inputs.to(device)\n",
        "            decoder_inputs = decoder_inputs.to(device)\n",
        "            decoder_targets = decoder_targets.to(device)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = model(encoder_inputs, decoder_inputs) # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n",
        "\n",
        "            # 손실 계산\n",
        "            # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n",
        "            # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n",
        "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # 정확도 계산\n",
        "            mask = decoder_targets != 0 # 패딩 토큰 제외\n",
        "            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item() # dim=-1로 마지막 차원인 tar_vocab_size에 대해 최대가 되는 값의 인덱스가 decoder_target의 인덱스와 같은 경우\n",
        "            total_count += mask.sum().item() # 패딩 토큰을 제외한 개수\n",
        "\n",
        "    return total_loss / len(dataloader), total_correct / total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9eFCbserPwGY"
      },
      "outputs": [],
      "source": [
        "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
        "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
        "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
        "\n",
        "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
        "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
        "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI0Yg6yaPxvO",
        "outputId": "6ed25b76-bf05-4d9d-e24c-eee2ad0fe793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30 | Train Loss: 3.9382 | Train Acc: 0.4314 | Valid Loss: 3.0173 | Valid Acc: 0.5246\n",
            "Validation loss improved from inf to 3.0173. 체크포인트를 저장합니다.\n",
            "Epoch: 2/30 | Train Loss: 2.6273 | Train Acc: 0.5687 | Valid Loss: 2.4859 | Valid Acc: 0.5911\n",
            "Validation loss improved from 3.0173 to 2.4859. 체크포인트를 저장합니다.\n",
            "Epoch: 3/30 | Train Loss: 2.1317 | Train Acc: 0.6198 | Valid Loss: 2.1835 | Valid Acc: 0.6266\n",
            "Validation loss improved from 2.4859 to 2.1835. 체크포인트를 저장합니다.\n",
            "Epoch: 4/30 | Train Loss: 1.7875 | Train Acc: 0.6560 | Valid Loss: 1.9916 | Valid Acc: 0.6475\n",
            "Validation loss improved from 2.1835 to 1.9916. 체크포인트를 저장합니다.\n",
            "Epoch: 5/30 | Train Loss: 1.5176 | Train Acc: 0.6883 | Valid Loss: 1.8502 | Valid Acc: 0.6656\n",
            "Validation loss improved from 1.9916 to 1.8502. 체크포인트를 저장합니다.\n",
            "Epoch: 6/30 | Train Loss: 1.2885 | Train Acc: 0.7215 | Valid Loss: 1.7307 | Valid Acc: 0.6825\n",
            "Validation loss improved from 1.8502 to 1.7307. 체크포인트를 저장합니다.\n",
            "Epoch: 7/30 | Train Loss: 1.0851 | Train Acc: 0.7542 | Valid Loss: 1.6467 | Valid Acc: 0.6928\n",
            "Validation loss improved from 1.7307 to 1.6467. 체크포인트를 저장합니다.\n",
            "Epoch: 8/30 | Train Loss: 0.9165 | Train Acc: 0.7840 | Valid Loss: 1.5866 | Valid Acc: 0.7036\n",
            "Validation loss improved from 1.6467 to 1.5866. 체크포인트를 저장합니다.\n",
            "Epoch: 9/30 | Train Loss: 0.7762 | Train Acc: 0.8116 | Valid Loss: 1.5308 | Valid Acc: 0.7139\n",
            "Validation loss improved from 1.5866 to 1.5308. 체크포인트를 저장합니다.\n",
            "Epoch: 10/30 | Train Loss: 0.6605 | Train Acc: 0.8346 | Valid Loss: 1.5050 | Valid Acc: 0.7180\n",
            "Validation loss improved from 1.5308 to 1.5050. 체크포인트를 저장합니다.\n",
            "Epoch: 11/30 | Train Loss: 0.5586 | Train Acc: 0.8552 | Valid Loss: 1.4874 | Valid Acc: 0.7196\n",
            "Validation loss improved from 1.5050 to 1.4874. 체크포인트를 저장합니다.\n",
            "Epoch: 12/30 | Train Loss: 0.4823 | Train Acc: 0.8706 | Valid Loss: 1.4815 | Valid Acc: 0.7232\n",
            "Validation loss improved from 1.4874 to 1.4815. 체크포인트를 저장합니다.\n",
            "Epoch: 13/30 | Train Loss: 0.4209 | Train Acc: 0.8814 | Valid Loss: 1.4729 | Valid Acc: 0.7256\n",
            "Validation loss improved from 1.4815 to 1.4729. 체크포인트를 저장합니다.\n",
            "Epoch: 14/30 | Train Loss: 0.3716 | Train Acc: 0.8903 | Valid Loss: 1.4793 | Valid Acc: 0.7275\n",
            "Epoch: 15/30 | Train Loss: 0.3339 | Train Acc: 0.8962 | Valid Loss: 1.4898 | Valid Acc: 0.7254\n",
            "Epoch: 16/30 | Train Loss: 0.3116 | Train Acc: 0.8998 | Valid Loss: 1.4886 | Valid Acc: 0.7288\n",
            "Epoch: 17/30 | Train Loss: 0.2878 | Train Acc: 0.9034 | Valid Loss: 1.5011 | Valid Acc: 0.7265\n",
            "Epoch: 18/30 | Train Loss: 0.2734 | Train Acc: 0.9051 | Valid Loss: 1.5148 | Valid Acc: 0.7277\n",
            "Epoch: 19/30 | Train Loss: 0.2538 | Train Acc: 0.9082 | Valid Loss: 1.5262 | Valid Acc: 0.7279\n",
            "Epoch: 20/30 | Train Loss: 0.2461 | Train Acc: 0.9088 | Valid Loss: 1.5331 | Valid Acc: 0.7305\n",
            "Epoch: 21/30 | Train Loss: 0.2325 | Train Acc: 0.9100 | Valid Loss: 1.5433 | Valid Acc: 0.7282\n",
            "Epoch: 22/30 | Train Loss: 0.2293 | Train Acc: 0.9107 | Valid Loss: 1.5598 | Valid Acc: 0.7284\n",
            "Epoch: 23/30 | Train Loss: 0.2199 | Train Acc: 0.9113 | Valid Loss: 1.5648 | Valid Acc: 0.7289\n",
            "Epoch: 24/30 | Train Loss: 0.2151 | Train Acc: 0.9117 | Valid Loss: 1.5699 | Valid Acc: 0.7278\n",
            "Epoch: 25/30 | Train Loss: 0.2076 | Train Acc: 0.9130 | Valid Loss: 1.5885 | Valid Acc: 0.7265\n",
            "Epoch: 26/30 | Train Loss: 0.2050 | Train Acc: 0.9129 | Valid Loss: 1.5925 | Valid Acc: 0.7279\n",
            "Epoch: 27/30 | Train Loss: 0.2022 | Train Acc: 0.9135 | Valid Loss: 1.5980 | Valid Acc: 0.7286\n",
            "Epoch: 28/30 | Train Loss: 0.2003 | Train Acc: 0.9131 | Valid Loss: 1.6142 | Valid Acc: 0.7272\n",
            "Epoch: 29/30 | Train Loss: 0.1995 | Train Acc: 0.9126 | Valid Loss: 1.6162 | Valid Acc: 0.7286\n",
            "Epoch: 30/30 | Train Loss: 0.1977 | Train Acc: 0.9128 | Valid Loss: 1.6241 | Valid Acc: 0.7285\n"
          ]
        }
      ],
      "source": [
        "# 학습 설정\n",
        "num_epochs = 30\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # 훈련 모드\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    total_train_correct = 0\n",
        "    total_train_count = 0\n",
        "    for encoder_inputs, decoder_inputs, decoder_targets in train_dataloader:\n",
        "        encoder_inputs = encoder_inputs.to(device)\n",
        "        decoder_inputs = decoder_inputs.to(device)\n",
        "        decoder_targets = decoder_targets.to(device)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 순방향 전파\n",
        "        # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n",
        "        outputs = model(encoder_inputs, decoder_inputs)\n",
        "\n",
        "        # 손실 계산 및 역방향 전파\n",
        "        # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n",
        "        # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n",
        "        loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # 가중치 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # train_loss 및 accuracy 계산\n",
        "        total_train_loss += loss.item()\n",
        "        mask = decoder_targets != 0  # 패딩 토큰 제외\n",
        "        total_train_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n",
        "        total_train_count += mask.sum().item()\n",
        "\n",
        "    # train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device) # 기존의 코드는 train_dataloader를 한 epoch당 두 번 조회하는 문제가 있다\n",
        "    train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_acc = total_train_correct / total_train_count\n",
        "    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
        "\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
        "\n",
        "    # 검증 손실이 최소일 때 체크포인트 저장\n",
        "    if valid_loss < best_val_loss:\n",
        "        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n",
        "        best_val_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFshJLSiP0ty",
        "outputId": "28528686-c3ef-43c6-fcb7-b3807bdb55b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model validation loss: 1.4729\n",
            "Best model validation accuracy: 0.7256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-ccccf4da9b07>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n"
          ]
        }
      ],
      "source": [
        "# 모델 로드\n",
        "model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
        "\n",
        "# 모델을 device에 올립니다.\n",
        "model.to(device)\n",
        "\n",
        "# 검증 데이터에 대한 정확도와 손실 계산\n",
        "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
        "\n",
        "print(f'Best model validation loss: {val_loss:.4f}')\n",
        "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFvMYyLXP2w_",
        "outputId": "0dc17b00-baaf-4066-ec42-91a6ee320bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "print(tar_vocab['<sos>'])\n",
        "print(tar_vocab['<eos>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5uUF7G9P67R"
      },
      "source": [
        "## Machine Translation (Inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7h1-2RjjP8Zk"
      },
      "outputs": [],
      "source": [
        "index_to_src = {v: k for k, v in src_vocab.items()}\n",
        "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
        "\n",
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0): # <PAD> 토큰은 제외\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_vocab['<sos>'] and encoded_word != tar_vocab['<eos>']): # <PAD>, <sos>, <eos>는 제외\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKbSVeSZQFhe",
        "outputId": "a82d25f1-50da-4a77-e5b3-8ac397b883e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   9   15   10 1949    5    0    0]\n",
            "[   3   39  183   14   37   33 1126    7    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  39  183   14   37   33 1126    7    4    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input_test[25])\n",
        "print(decoder_input_test[25])\n",
        "print(decoder_target_test[25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "t6k71sCEQGoC"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, max_output_len, int_to_src_token, int_to_tar_token):\n",
        "    # input_seq: 입력 문장을 정수 인코딩한 시퀀스/ input_seq.shape = [seq_length]\n",
        "    encoder_inputs = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    # encoder_inputs.shape == [batch_size, sequence_length]이고 batch_size는 1이 된다\n",
        "\n",
        "    # 인코더의 초기 상태 설정\n",
        "    hidden, cell = model.encoder(encoder_inputs)\n",
        "\n",
        "    # 시작 토큰 <sos>을 디코더의 첫 입력으로 설정\n",
        "    # unsqueeze(0)는 배치 차원을 추가하기 위함.\n",
        "    decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device) # [[3]]의 형태로 들어가게 됨(shape = [batch_size, 1])\n",
        "\n",
        "    decoded_tokens = []\n",
        "\n",
        "    # for문을 도는 것 == 디코더의 각 시점\n",
        "    for _ in range(max_output_len):\n",
        "        output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
        "\n",
        "        # 소프트맥스 회귀를 수행. 예측 단어의 인덱스\n",
        "        output_token = output.argmax(dim=-1).item()\n",
        "\n",
        "        # 종료 토큰 <eos>\n",
        "        if output_token == 4:\n",
        "            break\n",
        "\n",
        "        # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴합니다.\n",
        "        decoded_tokens.append(output_token)\n",
        "\n",
        "        # 현재 시점의 예측. 다음 시점의 입력으로 사용된다.\n",
        "        decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    return ' '.join(int_to_tar_token[token] for token in decoded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdpssGTiQIBt",
        "outputId": "8b373ec3-20ef-4e70-cd9e-6ef47ac935fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : i m a humble guy . \n",
            "정답문장 : je suis un type humble . \n",
            "번역문장 : je suis un mec intelligent .\n",
            "--------------------------------------------------\n",
            "입력문장 : that ll be my job . \n",
            "정답문장 : ce sera mon travail . \n",
            "번역문장 : ce sera mon travail .\n",
            "--------------------------------------------------\n",
            "입력문장 : just be happy . \n",
            "정답문장 : sois tout simplement heureuse . \n",
            "번역문장 : soyez tout simplement heureux .\n",
            "--------------------------------------------------\n",
            "입력문장 : i m always hungry . \n",
            "정답문장 : j ai tout le temps faim . \n",
            "번역문장 : j ai toujours faim .\n",
            "--------------------------------------------------\n",
            "입력문장 : did tom fall ? \n",
            "정답문장 : tom est il tombe ? \n",
            "번역문장 : tom est il tombe ?\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_train[seq_index]\n",
        "  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"번역문장 :\",translated_text)\n",
        "  print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tTqQbJ_QJde",
        "outputId": "2732c826-da59-4f4d-e61b-062aaac8c8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : go wait outside . \n",
            "정답문장 : va attendre dehors . \n",
            "번역문장 : allez attraper dehors !\n",
            "--------------------------------------------------\n",
            "입력문장 : tom is staying . \n",
            "정답문장 : tom reste . \n",
            "번역문장 : tom hurla .\n",
            "--------------------------------------------------\n",
            "입력문장 : you re charming . \n",
            "정답문장 : tu es charmant . \n",
            "번역문장 : vous etes charmante .\n",
            "--------------------------------------------------\n",
            "입력문장 : do you know us ? \n",
            "정답문장 : nous connaissez vous ? \n",
            "번역문장 : tu connais nous ?\n",
            "--------------------------------------------------\n",
            "입력문장 : get out of the car . \n",
            "정답문장 : sors de la voiture ! \n",
            "번역문장 : sortez de la camionnette !\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_test[seq_index]\n",
        "  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"번역문장 :\",translated_text)\n",
        "  print(\"-\"*50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "17xj3TusNvIu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtnr-w8XOVbd",
        "outputId": "d85a8f46-2832-4f68-e65e-0b0c9f202630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-08-30 05:37:54--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  3.92MB/s    in 1.9s    \n",
            "\n",
            "2024-08-30 05:37:56 (3.92 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ]
        }
      ],
      "source": [
        "# 기계 번역에 사용할 데이터셋은 왼쪽의 영어 문장과 오른쪽의 프랑스어 문장을 tab으로 구분한 형식(19만개의 병렬 문장 샘플)\n",
        "!wget -c http://www.manythings.org/anki/fra-eng.zip && unzip -o fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uJkqNFlaOX7k"
      },
      "outputs": [],
      "source": [
        "def unicode_to_ascii(s):\n",
        "  # 프랑스어 악센트(accent) 삭제\n",
        "  # 예시 : 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QBNeJmQNOZfY"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sent):\n",
        "  # 악센트 삭제 함수 호출\n",
        "  sent = unicode_to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백을 만듭니다.\n",
        "  # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XlGih0rjObEi"
      },
      "outputs": [],
      "source": [
        "num_samples = 33000\n",
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t') # tab을 기준으로 source(영어)와 target(프랑스어)로 구분\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()] # Decoder의 입력(프랑스어 입력)으로는 <sos>가 맨 앞에 추가된다\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()] # Decoder의 출력(프랑스어 출력)으로는 <eos>가 마지막에 추가된다\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRkhuQmZOc4J",
        "outputId": "1dfb51e6-6b1b-4401-cfb0-0a1dec338e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
          ]
        }
      ],
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D0HMayt2O33k"
      },
      "outputs": [],
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO83jWzSO88Q",
        "outputId": "e35c0970-c89b-4255-c450-97e3cb239d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
            "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
            "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
          ]
        }
      ],
      "source": [
        "print('인코더의 입력 :',sents_en_in[:5])\n",
        "print('디코더의 입력 :',sents_fra_in[:5])\n",
        "print('디코더의 레이블 :',sents_fra_out[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6B26J9n3PGKm"
      },
      "outputs": [],
      "source": [
        "def build_vocab(sents):\n",
        "  # 사전 만들기\n",
        "  word_list = []\n",
        "  for sent in sents:\n",
        "      for word in sent:\n",
        "        word_list.append(word)\n",
        "\n",
        "  # 각 단어별 등장 빈도를 계산하여 등장 빈도가 높은 순서로 정렬\n",
        "  word_counts = Counter(word_list)\n",
        "  vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "  # 사전을 바탕으로 단어를 정수 인덱스로 인코딩하는 딕셔너리 생성\n",
        "  word_to_index = {}\n",
        "  word_to_index['<PAD>'] = 0\n",
        "  word_to_index['<UNK>'] = 1\n",
        "\n",
        "  # 등장 빈도가 높은 단어일수록 낮은 정수를 부여\n",
        "  for index, word in enumerate(vocab) :\n",
        "    word_to_index[word] = index + 2\n",
        "\n",
        "  return word_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP5XdVblPX6z",
        "outputId": "fedd70f9-91e0-4c93-98b3-963213990a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "영어 단어 집합의 크기 : 4486, 프랑스어 단어 집합의 크기 : 7879\n"
          ]
        }
      ],
      "source": [
        "src_vocab = build_vocab(sents_en_in)\n",
        "tar_vocab = build_vocab(sents_fra_in + sents_fra_out) # 프랑스어 입력과 프랑스어 출력 문장들로 vocab을 만들고 정수 인덱스 인코딩을 만든다\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "tar_vocab_size = len(tar_vocab)\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O2Fl176UPZLw"
      },
      "outputs": [],
      "source": [
        "# 영어와 프랑스어 모두 word_to_index의 반대인 정수 인덱스로부터 원래의 문장을 복원(Decoding)하는 딕셔너리 생성\n",
        "index_to_src = {v: k for k, v in src_vocab.items()}\n",
        "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
        "\n",
        "def texts_to_sequences(sents, word_to_index):\n",
        "  encoded_X_data = []\n",
        "  for sent in tqdm(sents):\n",
        "    index_sequences = []\n",
        "    for word in sent:\n",
        "      try:\n",
        "          index_sequences.append(word_to_index[word])\n",
        "      except KeyError: # 사전에 없는 단어는 Unk 토큰의 정수 인덱스(1번)으로 변환\n",
        "          index_sequences.append(word_to_index['<UNK>'])\n",
        "    encoded_X_data.append(index_sequences)\n",
        "  return encoded_X_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4I6JyT3Paci",
        "outputId": "e04f6f06-d9b4-4c46-c4b4-164754a4ec57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 33000/33000 [00:00<00:00, 400020.90it/s]\n",
            "100%|██████████| 33000/33000 [00:00<00:00, 376136.01it/s]\n",
            "100%|██████████| 33000/33000 [00:00<00:00, 110560.34it/s]\n"
          ]
        }
      ],
      "source": [
        "encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n",
        "decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n",
        "decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oA1o5pdPblr",
        "outputId": "5f6cfb8d-60ec-477b-9776-f116db745a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [736, 2]\n"
          ]
        }
      ],
      "source": [
        "# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n",
        "# 인코더 입력이므로 <sos>나 <eos>가 없음\n",
        "for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n",
        "    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fkUn1iENPc3-"
      },
      "outputs": [],
      "source": [
        "def pad_sequences(sentences, max_len=None):\n",
        "    # 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n",
        "    if max_len is None:\n",
        "        max_len = max([len(sentence) for sentence in sentences])\n",
        "\n",
        "    # 최대길이까지 0으로 초기화(0은 PAD 토큰)\n",
        "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
        "    for index, sentence in enumerate(sentences):\n",
        "        if len(sentence) != 0:\n",
        "            features[index, :len(sentence)] = np.array(sentence)[:max_len] # 최대 길이만큼 기존 문장의 인코딩된 정수 인덱스 sequence를 덮씌운다\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-0lkT1_Pfvy",
        "outputId": "58f726fe-ab3b-442a-fea9-e0a8e34d99d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인코더의 입력의 크기(shape) : (33000, 7)\n",
            "디코더의 입력의 크기(shape) : (33000, 16)\n",
            "디코더의 레이블의 크기(shape) : (33000, 16)\n"
          ]
        }
      ],
      "source": [
        "# 문장들을 정수 인덱스들의 시퀀스로 변환해둔 상태\n",
        "# 이번에는 padding을 추가하여 길이를 맞춰준다\n",
        "encoder_input = pad_sequences(encoder_input)\n",
        "decoder_input = pad_sequences(decoder_input)\n",
        "decoder_target = pad_sequences(decoder_target)\n",
        "\n",
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaMUu5kPPgzP",
        "outputId": "b06e65db-2560-4ecd-b246-35c5f869c527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "랜덤 시퀀스 : [16800 27393 25265 ... 11386 29099 27082]\n"
          ]
        }
      ],
      "source": [
        "indices = np.arange(encoder_input.shape[0]) # 33000을 범위로 하는 index 리스트를 만들고\n",
        "np.random.shuffle(indices)                  # shuffle해서 뒤섞는다\n",
        "print('랜덤 시퀀스 :',indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtqJpgiKPi4f",
        "outputId": "6a0c39dc-f044-40e7-ea06-20941ce4fb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['they', 'deserve', 'more', '.', '<PAD>', '<PAD>', '<PAD>']\n",
            "['<sos>', 'ils', 'meritent', 'plus', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['ils', 'meritent', 'plus', '.', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ],
      "source": [
        "# 뒤섞은 index를 이용해서 encoder_input, decoder_input, decoder_target을 모두 뒤섞어준다\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n",
        "\n",
        "# 랜덤한 인덱스에 대해 값을 찍어본 결과\n",
        "print([index_to_src[word] for word in encoder_input[30997]]) # encoder의 입력(영어 문장)\n",
        "print([index_to_tar[word] for word in decoder_input[30997]]) # decoder의 입력(<sos>로 시작하는 프랑스어 문장)\n",
        "print([index_to_tar[word] for word in decoder_target[30997]]) # decoder의 출력(<eos>로 끝나는 프랑스어 문장)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDwZXlVPlU8",
        "outputId": "c471e842-20e8-41ad-8821-37ae4ba40424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "검증 데이터의 개수 : 3300\n"
          ]
        }
      ],
      "source": [
        "n_of_val = int(33000*0.1)\n",
        "print('검증 데이터의 개수 :',n_of_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MbLN9dQ4Pnel"
      },
      "outputs": [],
      "source": [
        "# 뒤에서 3300개 전까지를 train으로 잡고\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "# 뒤의 3300개를 test로 잡는다\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k3JRz3kPn6I",
        "outputId": "ec9909d2-0aa2-4361-e8a2-042bbf750007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 source 데이터의 크기 : (29700, 7)\n",
            "훈련 target 데이터의 크기 : (29700, 16)\n",
            "훈련 target 레이블의 크기 : (29700, 16)\n",
            "테스트 source 데이터의 크기 : (3300, 7)\n",
            "테스트 target 데이터의 크기 : (3300, 16)\n",
            "테스트 target 레이블의 크기 : (3300, 16)\n"
          ]
        }
      ],
      "source": [
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fNtfGqVPrIw"
      },
      "source": [
        "## Machine Translation (Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ptvan86aPpcC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "embedding_dim = 256 # Seq2Seq를 이루는 LSTM에 입력으로 들어가기 전 단어를 256차원의 임베딩 값으로 변환\n",
        "hidden_units = 256  # LSTM의 각 time step의 hidden state의 dimension(=hidden_size)\n",
        "\n",
        "# Encoder의 경우 Seq2Seq without Attention과 동일\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0) # 0번 인덱스를 padding으로\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # x.shape == [batch_size, seq_len, embedding_dim]\n",
        "        outputs, (hidden, cell) = self.lstm(x)\n",
        "        # outputs.shape == (batch_size, seq_length, hidden_size) : LSTM의 모든 time step에서의 출력 벡터(시퀀스의 각 시점에서 계산된 hidden state)\n",
        "        # hidden.shape == (1, batch_size, hidden_units)          : 시퀀스의 마지막 time step에서의 hidden state 값\n",
        "        # cell.shape == (1, batch_size, hidden_units)            : 시퀀스의 마지막 time step에서의 cell state 값\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "# Decoder의 경우 Attention 매커니즘이 적용되기 때문에 달라진다\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, tar_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx=0)\n",
        "        # Bahdanau Attention의 경우 LSTM의 입력에 이전 state로 계산한 Attention Vector가 Concat되기 때문에 입력 차원이 embedding_dim(target 문장의 임베딩) + hidden_units(attention vector)가 된다\n",
        "        self.lstm = nn.LSTM(embedding_dim + hidden_units, hidden_units, batch_first=True)\n",
        "        # 출력층(Fully Connected Layer)에서는 LSTM의 hidden state를 입력으로 target인 프랑스어 어휘 사전에 대한 로짓값을 계산하여 반환한다\n",
        "        self.fc = nn.Linear(hidden_units, tar_vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=1) # dim=1는 [batch, seq_length, feature_dim]이 있을 때 seq_length에 대해 softmax를 취한다는 것\n",
        "\n",
        "    def forward(self, x, encoder_outputs, hidden, cell):\n",
        "        x = self.embedding(x) # [batch_size, seq_length]인 x를 입력으로 임베딩하여 [batch_size, seq_length, embedding_dim]을 생성한다\n",
        "\n",
        "        # Dot product attention\n",
        "        attention_scores = torch.bmm(encoder_outputs, hidden.transpose(0, 1).transpose(1, 2)) # attention_scores.shape: (batch_size, source_seq_len, 1)\n",
        "        # bmm은 배치 단위로 3D 텐서 간의 행렬 곱셈을 수행 --> [batch_size, n, m], [batch_size, m, p]인 텐서에 대해 [batch_size, n, p]를 반환한다\n",
        "        # encoder_outputs은 (batch_size, seq_length, hidden_size)이고 hidden은 (1, batch_size, hidden_units)이다\n",
        "        # hidden.transpose(0, 1)로 (batch_size, 1, hidden_units)로 batch_size를 맨 앞으로 빼고\n",
        "        # hidden.transpose(0, 1).transpose(1, 2)로 (batch_size, hidden_units, 1)로 hidden_units을 두 번째로 뺀다\n",
        "        # hidden은 이전 Decoder의 이전 state를 의미하여 encoder_outputs은 Encoder의 모든 time step의 hidden state를 의미한다\n",
        "        # Decoder의 이전 state와 Encoder의 모든 time step의 hidden state를 행렬 곱(Dot Product)하여 Encoder 모든 time step 각각에 대한 attention_score를 계산하게 된다\n",
        "\n",
        "        attention_weights = self.softmax(attention_scores) # attention_weights.shape: (batch_size, source_seq_len, 1)\n",
        "        # [batch_size, encoder_seq_length, 1]인 attention_scores에 대해 dim=1 옵션인 softmax를 적용하면 encoder의 seq_length, 즉 모든 time step의 attention_score를 0~1 사이의 weight로 변환한다\n",
        "\n",
        "        # context_vector.shape: (batch_size, 1, hidden_units)\n",
        "        context_vector = torch.bmm(attention_weights.transpose(1, 2), encoder_outputs)\n",
        "        # attention_weights.transpose(1, 2)는 [batch_size, 1, source_seq_length]이고 encoder_outputs은 [batch_size, seq_length, hidden_size]이기 때문에\n",
        "        # context_vector는 attention weights와 Encoder의 hidden state를 weighted sum한 것이 되어 [batch_size, 1, hidden_size]의 차원을 갖게 된다\n",
        "        # 그리고 이러한 context_vector는 Decoder의 이전 state(hidden)를 바탕으로 Encoder에 대해 attention한 정보를 갖고 있는 것이다\n",
        "\n",
        "        # context vector를 Decoder의 seq_length만큼 반복하여 늘린다\n",
        "        seq_len = x.shape[1] # x.shape[1]은 Decoder의 입력인 x(target sentence)의 sequence_length이다\n",
        "        context_vector_repeated = context_vector.repeat(1, seq_len, 1) # context_vector_repeated.shape: (batch_size, target_seq_len, hidden_units)\n",
        "\n",
        "        # Context Vector와 Decoder 입력의 임베딩을 Concat하여 새로운 입력으로 LSTM에 넣게 된다\n",
        "        x = torch.cat((x, context_vector_repeated), dim=2) # new_x.shape = [batch_size, target_seq_length, embedding_dim + hidden_units]\n",
        "\n",
        "        # Attention 매커니즘으로 얻은 정보와 현재 입력을 합쳐서 LSTM의 입력으로 넣고 Decoder의 출력(마지막 hidden state)을 계산해낸다\n",
        "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
        "\n",
        "        # Decoder의 LSTM 출력을 출력층(Fully Connected Layer)에 입력으로 넣어서 target인 프랑스 어휘에 대한 로짓값을 계산한다\n",
        "        output = self.fc(output) # output.shape: (batch_size, target_seq_len, tar_vocab_size)\n",
        "\n",
        "        return output, hidden, cell\n",
        "\n",
        "# Seq2Seq도 Attention이 있냐 없냐 상관 없이 동일하다\n",
        "# 즉, Attention 매커니즘은 Decoder에서만 적용된다는 것을 알 수 있다!!\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "        output, _, _ = self.decoder(trg, encoder_outputs, hidden, cell)\n",
        "        return output\n",
        "\n",
        "encoder = Encoder(src_vocab_size, embedding_dim, hidden_units)\n",
        "decoder = Decoder(tar_vocab_size, embedding_dim, hidden_units)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0) # PAD 토큰의 정수 인덱스인 0번에 대한 손실 계산은 무시\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6RhONZxPtXs",
        "outputId": "5f395d74-3544-47dd-e55a-e53480c735f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(4486, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(7879, 256, padding_idx=0)\n",
            "    (lstm): LSTM(512, 256, batch_first=True)\n",
            "    (fc): Linear(in_features=256, out_features=7879, bias=True)\n",
            "    (softmax): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PHGcTybqPuUN"
      },
      "outputs": [],
      "source": [
        "def evaluation(model, dataloader, loss_function, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    # gradient 계산 및 업데이트 없이\n",
        "    with torch.no_grad():\n",
        "        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n",
        "            # 데이터를 꺼내오고\n",
        "            encoder_inputs = encoder_inputs.to(device)\n",
        "            decoder_inputs = decoder_inputs.to(device)\n",
        "            decoder_targets = decoder_targets.to(device)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = model(encoder_inputs, decoder_inputs) # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n",
        "\n",
        "            # 손실 계산\n",
        "            # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n",
        "            # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n",
        "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # 정확도 계산\n",
        "            mask = decoder_targets != 0 # 패딩 토큰 제외\n",
        "            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item() # dim=-1로 마지막 차원인 tar_vocab_size에 대해 최대가 되는 값의 인덱스가 decoder_target의 인덱스와 같은 경우\n",
        "            total_count += mask.sum().item() # 패딩 토큰을 제외한 개수\n",
        "\n",
        "    return total_loss / len(dataloader), total_correct / total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9eFCbserPwGY"
      },
      "outputs": [],
      "source": [
        "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
        "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
        "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
        "\n",
        "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
        "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
        "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI0Yg6yaPxvO",
        "outputId": "b487a801-2812-4302-a60d-a2cdef141505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/30 | Train Loss: 3.8506 | Train Acc: 0.4421 | Valid Loss: 2.9631 | Valid Acc: 0.5353\n",
            "Validation loss improved from inf to 2.9631. 체크포인트를 저장합니다.\n",
            "Epoch: 2/30 | Train Loss: 2.5842 | Train Acc: 0.5735 | Valid Loss: 2.4307 | Valid Acc: 0.5991\n",
            "Validation loss improved from 2.9631 to 2.4307. 체크포인트를 저장합니다.\n",
            "Epoch: 3/30 | Train Loss: 2.0864 | Train Acc: 0.6252 | Valid Loss: 2.1305 | Valid Acc: 0.6294\n",
            "Validation loss improved from 2.4307 to 2.1305. 체크포인트를 저장합니다.\n",
            "Epoch: 4/30 | Train Loss: 1.7290 | Train Acc: 0.6646 | Valid Loss: 1.9387 | Valid Acc: 0.6532\n",
            "Validation loss improved from 2.1305 to 1.9387. 체크포인트를 저장합니다.\n",
            "Epoch: 5/30 | Train Loss: 1.4454 | Train Acc: 0.7005 | Valid Loss: 1.7932 | Valid Acc: 0.6716\n",
            "Validation loss improved from 1.9387 to 1.7932. 체크포인트를 저장합니다.\n",
            "Epoch: 6/30 | Train Loss: 1.2055 | Train Acc: 0.7364 | Valid Loss: 1.6781 | Valid Acc: 0.6879\n",
            "Validation loss improved from 1.7932 to 1.6781. 체크포인트를 저장합니다.\n",
            "Epoch: 7/30 | Train Loss: 1.0013 | Train Acc: 0.7723 | Valid Loss: 1.6043 | Valid Acc: 0.6972\n",
            "Validation loss improved from 1.6781 to 1.6043. 체크포인트를 저장합니다.\n",
            "Epoch: 8/30 | Train Loss: 0.8338 | Train Acc: 0.7998 | Valid Loss: 1.5362 | Valid Acc: 0.7069\n",
            "Validation loss improved from 1.6043 to 1.5362. 체크포인트를 저장합니다.\n",
            "Epoch: 9/30 | Train Loss: 0.6902 | Train Acc: 0.8291 | Valid Loss: 1.4884 | Valid Acc: 0.7146\n",
            "Validation loss improved from 1.5362 to 1.4884. 체크포인트를 저장합니다.\n",
            "Epoch: 10/30 | Train Loss: 0.5793 | Train Acc: 0.8506 | Valid Loss: 1.4696 | Valid Acc: 0.7195\n",
            "Validation loss improved from 1.4884 to 1.4696. 체크포인트를 저장합니다.\n",
            "Epoch: 11/30 | Train Loss: 0.4884 | Train Acc: 0.8693 | Valid Loss: 1.4566 | Valid Acc: 0.7192\n",
            "Validation loss improved from 1.4696 to 1.4566. 체크포인트를 저장합니다.\n",
            "Epoch: 12/30 | Train Loss: 0.4225 | Train Acc: 0.8804 | Valid Loss: 1.4563 | Valid Acc: 0.7213\n",
            "Validation loss improved from 1.4566 to 1.4563. 체크포인트를 저장합니다.\n",
            "Epoch: 13/30 | Train Loss: 0.3746 | Train Acc: 0.8896 | Valid Loss: 1.4518 | Valid Acc: 0.7249\n",
            "Validation loss improved from 1.4563 to 1.4518. 체크포인트를 저장합니다.\n",
            "Epoch: 14/30 | Train Loss: 0.3297 | Train Acc: 0.8968 | Valid Loss: 1.4582 | Valid Acc: 0.7259\n",
            "Epoch: 15/30 | Train Loss: 0.2988 | Train Acc: 0.9018 | Valid Loss: 1.4681 | Valid Acc: 0.7250\n",
            "Epoch: 16/30 | Train Loss: 0.2770 | Train Acc: 0.9050 | Valid Loss: 1.4769 | Valid Acc: 0.7262\n",
            "Epoch: 17/30 | Train Loss: 0.2639 | Train Acc: 0.9060 | Valid Loss: 1.4928 | Valid Acc: 0.7274\n",
            "Epoch: 18/30 | Train Loss: 0.2510 | Train Acc: 0.9079 | Valid Loss: 1.5075 | Valid Acc: 0.7246\n",
            "Epoch: 19/30 | Train Loss: 0.2398 | Train Acc: 0.9092 | Valid Loss: 1.5083 | Valid Acc: 0.7264\n",
            "Epoch: 20/30 | Train Loss: 0.2332 | Train Acc: 0.9098 | Valid Loss: 1.5182 | Valid Acc: 0.7275\n",
            "Epoch: 21/30 | Train Loss: 0.2235 | Train Acc: 0.9113 | Valid Loss: 1.5356 | Valid Acc: 0.7268\n",
            "Epoch: 22/30 | Train Loss: 0.2174 | Train Acc: 0.9125 | Valid Loss: 1.5450 | Valid Acc: 0.7262\n",
            "Epoch: 23/30 | Train Loss: 0.2170 | Train Acc: 0.9114 | Valid Loss: 1.5604 | Valid Acc: 0.7248\n",
            "Epoch: 24/30 | Train Loss: 0.2087 | Train Acc: 0.9122 | Valid Loss: 1.5617 | Valid Acc: 0.7272\n",
            "Epoch: 25/30 | Train Loss: 0.2039 | Train Acc: 0.9127 | Valid Loss: 1.5749 | Valid Acc: 0.7261\n",
            "Epoch: 26/30 | Train Loss: 0.2009 | Train Acc: 0.9126 | Valid Loss: 1.5864 | Valid Acc: 0.7267\n",
            "Epoch: 27/30 | Train Loss: 0.1986 | Train Acc: 0.9129 | Valid Loss: 1.5920 | Valid Acc: 0.7262\n",
            "Epoch: 28/30 | Train Loss: 0.1964 | Train Acc: 0.9131 | Valid Loss: 1.6004 | Valid Acc: 0.7243\n",
            "Epoch: 29/30 | Train Loss: 0.1980 | Train Acc: 0.9127 | Valid Loss: 1.6057 | Valid Acc: 0.7245\n",
            "Epoch: 30/30 | Train Loss: 0.2005 | Train Acc: 0.9125 | Valid Loss: 1.6130 | Valid Acc: 0.7242\n"
          ]
        }
      ],
      "source": [
        "# 학습 설정\n",
        "num_epochs = 30\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # 훈련 모드\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    total_train_correct = 0\n",
        "    total_train_count = 0\n",
        "    for encoder_inputs, decoder_inputs, decoder_targets in train_dataloader:\n",
        "        encoder_inputs = encoder_inputs.to(device)\n",
        "        decoder_inputs = decoder_inputs.to(device)\n",
        "        decoder_targets = decoder_targets.to(device)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 순방향 전파\n",
        "        # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n",
        "        outputs = model(encoder_inputs, decoder_inputs)\n",
        "\n",
        "        # 손실 계산 및 역방향 전파\n",
        "        # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n",
        "        # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n",
        "        loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # 가중치 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # train_loss 및 accuracy 계산\n",
        "        total_train_loss += loss.item()\n",
        "        mask = decoder_targets != 0  # 패딩 토큰 제외\n",
        "        total_train_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n",
        "        total_train_count += mask.sum().item()\n",
        "\n",
        "    # train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device) # 기존의 코드는 train_dataloader를 한 epoch당 두 번 조회하는 문제가 있다\n",
        "    train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_acc = total_train_correct / total_train_count\n",
        "    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
        "\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
        "\n",
        "    # 검증 손실이 최소일 때 체크포인트 저장\n",
        "    if valid_loss < best_val_loss:\n",
        "        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n",
        "        best_val_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFshJLSiP0ty",
        "outputId": "21f80fa9-96d5-456f-ced4-50ef171e6f3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-ccccf4da9b07>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model validation loss: 1.4518\n",
            "Best model validation accuracy: 0.7249\n"
          ]
        }
      ],
      "source": [
        "# 모델 로드\n",
        "model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
        "\n",
        "# 모델을 device에 올립니다.\n",
        "model.to(device)\n",
        "\n",
        "# 검증 데이터에 대한 정확도와 손실 계산\n",
        "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
        "\n",
        "print(f'Best model validation loss: {val_loss:.4f}')\n",
        "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFvMYyLXP2w_",
        "outputId": "d6c0315a-1e4d-44fc-c280-b1a80442292f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "print(tar_vocab['<sos>'])\n",
        "print(tar_vocab['<eos>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5uUF7G9P67R"
      },
      "source": [
        "## Machine Translation (Inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7h1-2RjjP8Zk"
      },
      "outputs": [],
      "source": [
        "index_to_src = {v: k for k, v in src_vocab.items()}\n",
        "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
        "\n",
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0): # <PAD> 토큰은 제외\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_vocab['<sos>'] and encoded_word != tar_vocab['<eos>']): # <PAD>, <sos>, <eos>는 제외\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKbSVeSZQFhe",
        "outputId": "83f20599-091f-43fc-b82c-242232e406cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[184  14  12 207   5   0   0]\n",
            "[   3   20   38   15   12 2404    7    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  20   38   15   12 2404    7    4    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input_test[25])\n",
        "print(decoder_input_test[25])\n",
        "print(decoder_target_test[25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "t6k71sCEQGoC"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, max_output_len, int_to_src_token, int_to_tar_token):\n",
        "    # input_seq: 입력 문장을 정수 인코딩한 시퀀스/ input_seq.shape = [seq_length]\n",
        "    encoder_inputs = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    # encoder_inputs.shape == [batch_size, sequence_length]이고 batch_size는 1이 된다\n",
        "\n",
        "    # 인코더의 초기 상태 설정\n",
        "    encoder_outputs, hidden, cell = model.encoder(encoder_inputs)\n",
        "\n",
        "    # 시작 토큰 <sos>을 디코더의 첫 입력으로 설정\n",
        "    # unsqueeze(0)는 배치 차원을 추가하기 위함.\n",
        "    decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device) # [[3]]의 형태로 들어가게 됨(shape = [batch_size, 1])\n",
        "\n",
        "    decoded_tokens = []\n",
        "\n",
        "    # for문을 도는 것 == 디코더의 각 시점\n",
        "    for _ in range(max_output_len):\n",
        "        output, hidden, cell = model.decoder(decoder_input, encoder_outputs, hidden, cell)\n",
        "\n",
        "        # 소프트맥스 회귀를 수행. 예측 단어의 인덱스\n",
        "        output_token = output.argmax(dim=-1).item()\n",
        "\n",
        "        # 종료 토큰 <eos>\n",
        "        if output_token == 4:\n",
        "            break\n",
        "\n",
        "        # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴합니다.\n",
        "        decoded_tokens.append(output_token)\n",
        "\n",
        "        # 현재 시점의 예측. 다음 시점의 입력으로 사용된다.\n",
        "        decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    return ' '.join(int_to_tar_token[token] for token in decoded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdpssGTiQIBt",
        "outputId": "228a59c9-970d-4e1d-b84e-3591ab1191a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력문장 : you better hurry . \n",
            "정답문장 : tu ferais mieux de te magner ! \n",
            "번역문장 : tu ferais mieux de te grouiller !\n",
            "--------------------------------------------------\n",
            "입력문장 : what do bees eat ? \n",
            "정답문장 : que mangent les abeilles ? \n",
            "번역문장 : que mangent les abeilles ?\n",
            "--------------------------------------------------\n",
            "입력문장 : enough is enough . \n",
            "정답문장 : c en est assez ! \n",
            "번역문장 : assez c est assez !\n",
            "--------------------------------------------------\n",
            "입력문장 : don t buy that one . \n",
            "정답문장 : n achete pas ca . \n",
            "번역문장 : n achete pas ca .\n",
            "--------------------------------------------------\n",
            "입력문장 : get out of my life . \n",
            "정답문장 : sortez de ma vie ! \n",
            "번역문장 : sors de ma vie !\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_train[seq_index]\n",
        "  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"번역문장 :\",translated_text)\n",
        "  print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tTqQbJ_QJde",
        "outputId": "7bf43fbd-8c64-4a01-9af4-7dbdbaded563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력문장 : he kept his hat on . \n",
            "정답문장 : il garda son chapeau sur la tete . \n",
            "번역문장 : il a aime sa parole .\n",
            "--------------------------------------------------\n",
            "입력문장 : she s innocent . \n",
            "정답문장 : elle est innocente . \n",
            "번역문장 : elle est ingenue .\n",
            "--------------------------------------------------\n",
            "입력문장 : it s wrong to lie . \n",
            "정답문장 : c est mal de mentir . \n",
            "번역문장 : c est a tort de pleurer .\n",
            "--------------------------------------------------\n",
            "입력문장 : we know this song . \n",
            "정답문장 : cette chanson nous est familiere . \n",
            "번역문장 : nous connaissons cette chanson .\n",
            "--------------------------------------------------\n",
            "입력문장 : where is the book ? \n",
            "정답문장 : ou est le livre ? \n",
            "번역문장 : ou est le livret ?\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_test[seq_index]\n",
        "  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"번역문장 :\",translated_text)\n",
        "  print(\"-\"*50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
